Using etcd in Project Clearwater - part II
------------------------------------------
In a recent [blog post](Etcd_1.md) we looked at [etcd](https://coreos.com/etcd/), which we use as the store for our cluster and service configuration. This post looks in more detail at [clearwater-etcd](https://github.com/Metaswitch/clearwater-etcd), our wrapper around etcd to allow us to easily install, cluster and manage the deployment. Clearwater-etcd is a GitHub repository containing the clearwater-cluster-manager package, which is responsible for creating the cluster configuration files for the Cassandra, Memcached and Chronos clusters, and kicking the Clearwater processes to pick up changes to the files. It also holds the clearwater-config-manager package, which is responsible for the configuration files for shared configuration. The hard problem we had was that we wanted a new Clearwater node to join a cluster with minimal operator intervention. This meant that we needed to automatically reconfigure our data stores, and once all the data stores had the updated configuration, automatically resynchronise the data in the clusters.

###Clearwater-cluster-manager
Our solution was the clearwater-cluster-manager, which acts as distributed barrier using etcd. This is responsible for the Cassandra, Memcached and Chronos clusters. For example, when a Sprout node starts up, the clearwater-cluster-manager discovers any existing Memcached and Chronos clusters (by querying etcd). It then creates the correct configuration files for the Sprout node and kicks the Sprout and Chronos process to pick up the configuration. When a new Sprout node is added, the clearwater-cluster-manager process on the new node updates the stored information in etcd, triggering the clearwater-cluster-manager process on the other Sprout nodes to pick up the new cluster configuration as well. In order for this to work, the clearwater-cluster-manager process must have careful ordering (e.g. we want all the Chronos configuration files in the cluster to be updated before we start resynchronizing between them). The clearwater-cluster-manager stores in etcd a key-value pair for each cluster. The stored value is the IP address of each node in the cluster, paired with the current state of the node. The clearwater-cluster-manager polls the etcd cluster for updates to any keys it knows about, and then uses a [finite-state-machine](https://en.wikipedia.org/wiki/Finite-state_machine) (FSM) to understand any updates - it only allows certain actions to be taken when all nodes are in particular combinations of states (i.e. only resynchronize when all nodes know about the updated configuration). An example of the stored information is:

*   Key – “/clearwater/<site name>/sprout/clustering/chronos”
*   Value – {'IP address of Node 1' : 'Node state', 'IP address of Node 2' : 'Node state', …}

Let’s run through a scale up as an example. In this example, there are two Sprout nodes (A and B) in the Chronos cluster, and Sprout Node C is attempting to join it. When C starts up, the clearwater-cluster-manager queries etcd for the Sprout Chronos cluster. As A and B already exist, the entry will show that there are two nodes, both in a normal state (e.g. {'A’s IP address’ : 'Normal', 'B’s IP address’ : 'Normal'}). C then appends itself to the entry to show that it’s waiting to join (this is [CAS](https://en.wikipedia.org/wiki/Compare-and-swap)’d – if the append had failed then C would have re-read the state and tried again). We have this stage to allow any other nodes that want to join to do so, so the scale up can be done in one go. The clearwater-cluster-manager processes on A and B will note the updated value for the Chronos cluster, (e.g. {'A’s IP address': 'Normal', 'B’s IP address': 'Normal', 'C’s IP address': 'Waiting to join'}), but there’s nothing for them to do at this stage. C then waits for 30 seconds (to allow any other nodes to potentially join as well), then it re-reads the key and marks itself as joining the cluster. A and B will see this change again, and they will alter their entries in the etcd value to acknowledge that they’ve seen that C wants to join. Once all three nodes have acknowledged that a change is in process, they update their Chronos cluster configuration file with the updated configuration, and kick Chronos to pick up the new configuration. Each node also updates their entry in etcd to mark that they’ve updated their configuration. Once all the nodes are in the state where they’ve updated their configuration, each node then runs the Chronos resynchronization [process](https://github.com/Metaswitch/chronos/blob/dev/doc/scaling.md#rebalancing-timers). Once this has completed, each node updates its entry to be in the normal state (e.g. the final value is therefore {'A’s IP address’ : 'Normal', 'B’s IP address’ : 'Normal', 'C’s IP address’ : 'Normal'}). You can find the whole FSM at [https://github.com/Metaswitch/clearwater-etcd/blob/master/src/metaswitch/clearwater/cluster_manager/synchronization_fsm.py](https://github.com/Metaswitch/clearwater-etcd/blob/master/src/metaswitch/clearwater/cluster_manager/synchronization_fsm.py).

###Clearwater-config-manager
Clearwater-etcd also holds the clearwater-config-manager, which is responsible for the configuration files for shared configuration. This is much simpler than the clearwater-cluster-manager; each configuration file is stored in etcd under a key for that file (e.g. the contents of the /etc/clearwater/shared\_config are stored in etcd under the key ‘shared\_config’). If changes to one of the shared configuration files are uploaded to etcd (details for how to do this are [here](http://clearwater.readthedocs.org/en/latest/Upgrading_a_Clearwater_deployment/index.html)) from one node, then clearwater-config-manager processes on the other nodes will notice the changed value, and update the configuration files on their node to match.
