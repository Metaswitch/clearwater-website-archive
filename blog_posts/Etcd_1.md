Using etcd in Project Clearwater
--------------------------------
We recently (in the ‘For Whom the Bell Tolls release) added new function to automatically manage our service and cluster configuration.

**Why did we do it?** For a while, we’ve known that our configuration model is unwieldy. Most of our service configuration is stored in /etc/clearwater/config, and while this file was broadly the same on each node it wasn’t identical – this meant that there was lots of duplicated configuration but also plenty of scope for accidental misconfiguration if a user copies files between nodes. It also meant that changing a configuration option could mean making the same change on every node in your deployment. Finally, scaling a deployment up or down was quite an involved process, requiring multiple steps on each node. This was time consuming, prone to error and complex enough that it was quite difficult to automate. We therefore wanted something that could take care of this for us, and simplify the setup of a node.

**What did we do?** The answer was to use [etcd](https://coreos.com/etcd/) to hold our sharable configuration - etcd is a distributed store that uses a consensus protocol ([Raft](http://en.wikipedia.org/wiki/Raft_%28computer_science%29)) to ensure data is stored consistently across all nodes. We use etcd to store our cluster and service configuration. To make this easier, we’ve also added our own wrapper ([clearwater-etcd](https://github.com/Metaswitch/clearwater-etcd)) around etcd to allow us to easily install, cluster and manage the deployment with etcd. We’re going to do another blog post on this that describes clearwater-etcd in more detail – watch this space!

**So what's new?** We split up our configuration into three different types, and put two of them under etcd’s control:

*   Local – This is configuration which is unique to a node (e.g. the node’s local IP). This is typically set in /etc/clearwater/local\_config. You need to create this on each node
*   Service – This is configuration which is shared across the deployment (e.g. the home domain of the deployment, the BGCF routes). This is typically set in /etc/clearwater/shared\_config, and /etc/clearwater/.json. You need to create this once on one node, then etcd makes it available to the other nodes.
*   Cluster –This is configuration that controls the Memcached, Cassandra and Chronos clusters (e.g. /etc/clearwater/cluster\_settings). These are now entirely created and managed by etcd – you don’t need to configure these at all!

Now when a deployment is created, you only need to add the service configuration to one node (typically the first Sprout). You can then upload the configuration files into etcd, and they’re then available for all other nodes in the deployment to download and apply. To make a change to the configuration, you need to edit the configuration file on one node and upload the file to etcd. You then can apply the configuration on your other nodes (which will also take care of restarting any dependent services). The cluster configuration files are created by etcd, and they don’t need any manual configuration anymore. When a node joins or leaves a cluster etcd takes care of updating all the relevant configuration file on the nodes, and reloading any service to pick up the updated configuration files.

**How can you use this?** We strongly recommend that any new and existing Clearwater deployments use our new automated configuration and cluster management system. To migrate an existing system please follow the instructions [here](http://clearwater.readthedocs.org/en/latest/Migrating_To_etcd/index.html), and to install a new deployment follow the instructions [here](http://clearwater.readthedocs.org/en/latest/Manual_Install/index.html). We hope that you find this much easier to use; do let us know what you think!
