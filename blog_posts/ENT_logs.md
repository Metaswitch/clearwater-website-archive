Service Reliability and ENT Logs
--------------------------------
People expect their phone service to work reliably and, as an IMS core, this means Clearwater needs to work reliably. Despite our best efforts, there are bugs in all software products and, even if there weren't, there are also always going to be failure modes in the virtualization platform hosting Clearwater or in the surrounding IMS network. So how do we ensure the service is reliable? First of all, Clearwater has several mechanisms to catch and automatically recover from failures. These mechanisms include catching exceptions and trying to recover, using monit to restart failed processes, running as a cluster and retrying failed requests and blacklisting failed peers and attempting to route around them. However, there are times when Clearwater can't automatically recover from failures - in particular, when the failure is with something else in the network. In these cases, manual interaction is required and the first step is alerting the operator of the fault. For this, Clearwater exposes a range of [alarms](http://clearwater.readthedocs.org/en/latest/SNMP_Alarms) over SNMP using [RFC 3877](https://tools.ietf.org/html/rfc3877). We recommend that operators configure their Network Management System (NMS) to monitor these, and configure Clearwater to send SNMP TRAPs to the NMS using the [snmp\_ip configuration option](http://clearwater.readthedocs.org/en/latest/Clearwater_Configuration_Options_Reference) in /etc/clearwater/shared\_config. Alarms tell you that something isn't working, but don't tell you why. For that, Clearwater exposes logs - specifically Enhanced Node Troubleshooting (ENT) logs. These consist of

*   a unique identifier for the log that occurred
*   description - this tells you what happened
*   cause - the cause of the event
*   effect - the impact of the event, e.g. whether subscribers will still be able to make calls
*   action - what action to take to recover from the event (if anything).

Importantly, ENT logs are targeted at administrators who don't necessarily know how Clearwater works - they don't go into quite as much detail, but they are written in language that should be understandable to more people. Each ENT log is sent as a single message over syslog, with "@@" separating the description, cause, effect and action. For example, you might see the following log.

> 1005 - Description: Request for http://hs.example.com:8888/impu/sip%3A6505550065%40cw-ngv.com/reg-data to HTTP server 1.2.3.4 failed with error "Couldn't connect to server" (code 7). @@Cause: An HTTP request failed to the specified server with the specified error code. @@Effect: This condition impacts the ability to register, subscribe, or make a call. @@Action: (1). Check to see if the specified host has failed. (2). Check to see if there is TCP connectivity to the host by using ping and/or Wireshark.

By default, these logs appear in /var/log/syslog on the server on which the event occurred. However, for large-scale deployments containing lots of servers, logging into each server to retrieve its logs is inconvenient. To cater for this, all ENT logs are sent with facility "local6", so you can configure them to be sent to a remote server by adding a line as follows to your /etc/rsyslog.conf file.

> local6.* @<syslog-server>

Of course, there are times when the ENT logs aren't sufficient to troubleshoot the problem. For that situation, Project Clearwater is integrated with Metaswitchâ€™s commercial [Service Assurance Server (SAS) product](http://www.metaswitch.com/products/management-systems/service-assurance-server) - this provides administrators and tech support personal access to detailed call flow and logic information, stored for at least 7 days, for all calls through the system. Clearwater also exposes a range of [more detailed logs and troubleshooting methods](http://clearwater.readthedocs.org/en/latest/Troubleshooting_and_Recovery), but these often require more in-depth knowledge.
